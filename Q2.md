2. (4.0 points) Implement a k-nearest neighbors classifier for both Euclidean distance and
Cosine Similarity using the signature provided in starter.py. This algorithm may be
computationally intensive. To address this, you must use transform your data in some
manner (e.g., dimensionality reduction, mapping grayscale to binary, dimension scaling,
etc.) -- the exact method is up to you. This is an opportunity to be creative with feature
construction. Similarly, you are free to select your own hyper-parameters (e.g., K, the
number of observations to use, default labels, etc.). Please describe all of your design
choices and hyper-parameter selections in a paragraph. Once you are satisfied with
performance on the validation set, run your classifier on the test set and summarize results in
a 10x10 confusion matrix. Analyze your results in another paragraph.


Design Choices & Hyperparameter Selections:

K: The number of neighbors is set to 1. After used cross validation to test accuracy with different k, k = 1 had the highest accuracy, so set k = 1 for KNN.

Transformation: standardization (Z-score normalization) is used for dimension scaling. Tried PCA with different number of PC and normalization, normalization produced the best accuracy.

Distance Metrics: Both Euclidean distance and Cosine Similarity were used. For cosine similarity, it's converted into a distance measure (1 - similarity) to fit the KNN paradigm.

---------
euclidean without n_components -> 77.5 %
euclidean with n_components = 0.95 -> 77.0

cosine without n_components -> 1.5 %
cosine with n_components = 0.95 -> 

